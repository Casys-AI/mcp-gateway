{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCP Gateway with LLM Integration\n\nDemonstrates how LLMs use MCP tools to execute code safely.\n\n**What you'll learn:**\n- Start HTTP MCP server\n- LLM calls tools via MCP\n- Workflow orchestration with tool calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Install & Start MCP Gateway\n\n**First time setup (~2-3 minutes):**\n- Installs npm dependencies (30-60s)\n- Downloads BGE-M3 model (60-90s) - 2.2GB\n- Initializes database & GraphRAG engine\n\n**Subsequent runs:** Server starts in ~5 seconds (model is cached)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "// Start the real MCP Gateway server\nconsole.log(\"üöÄ Starting AgentCards MCP Gateway...\");\nconsole.log(\"\");\nconsole.log(\"üì¶ First time? This will:\");\nconsole.log(\"   1. Install npm dependencies (~30-60s)\");\nconsole.log(\"   2. Download BGE-M3 model (~60-90s, 2.2GB)\");\nconsole.log(\"   3. Initialize database & GraphRAG\");\nconsole.log(\"\");\nconsole.log(\"‚è≥ Please wait, this may take 2-3 minutes on first run...\");\nconsole.log(\"\");\n\nconst server = new Deno.Command(\"deno\", {\n  args: [\"run\", \"--allow-all\", \"../examples/server.ts\"],\n  stdout: \"piped\",\n  stderr: \"piped\"\n});\n\nconst serverProcess = server.spawn();\n\n// Read server output to show progress\nconst decoder = new TextDecoder();\nlet ready = false;\n\n// Stream output with timeout\nconst timeoutMs = 180000; // 3 minutes\nconst startTime = Date.now();\n\nwhile (!ready && (Date.now() - startTime) < timeoutMs) {\n  const buf = new Uint8Array(1024);\n  try {\n    const n = await serverProcess.stdout.read(buf);\n    if (n) {\n      const output = decoder.decode(buf.subarray(0, n));\n      console.log(output);\n      if (output.includes(\"‚úÖ MCP Gateway ready!\")) {\n        ready = true;\n      }\n    }\n  } catch (e) {\n    // Continue on read errors\n  }\n  await new Promise(r => setTimeout(r, 100));\n}\n\nif (!ready) {\n  console.log(\"\");\n  console.log(\"‚ö†Ô∏è  Server taking longer than expected. Checking health...\");\n}\n\n// Verify server is responding\nawait new Promise(r => setTimeout(r, 2000));\nconst health = await fetch(\"http://localhost:3000/health\");\nconst healthData = await health.json();\n\nconsole.log(\"\");\nconsole.log(\"‚úÖ MCP Gateway is ready!\");\nconsole.log(`   Status: ${healthData.status}`);\nconsole.log(`   Port: 3000`);"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: List Available MCP Tools\n\nQuery the MCP server using JSON-RPC 2.0 protocol:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "// MCP uses JSON-RPC 2.0 protocol\nconst response = await fetch(\"http://localhost:3000/message\", {\n  method: \"POST\",\n  headers: { \"Content-Type\": \"application/json\" },\n  body: JSON.stringify({\n    jsonrpc: \"2.0\",\n    id: 1,\n    method: \"tools/list\",\n    params: {}\n  })\n});\n\nconst { result } = await response.json();\nconst { tools } = result;\n\nconsole.log(`üìã Available MCP tools (${tools.length}):\\n`);\nfor (const tool of tools) {\n  console.log(`üîß ${tool.name}`);\n  console.log(`   ${tool.description.slice(0, 80)}...`);\n  console.log();\n}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Call Tool Directly\n\nTest tool execution without LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "// Call MCP tool using JSON-RPC\nconst response = await fetch(\"http://localhost:3000/message\", {\n  method: \"POST\",\n  headers: { \"Content-Type\": \"application/json\" },\n  body: JSON.stringify({\n    jsonrpc: \"2.0\",\n    id: 2,\n    method: \"tools/call\",\n    params: {\n      name: \"agentcards__agentcards_execute_code\",\n      arguments: {\n        code: \"return Array.from({length: 10}, (_, i) => i * i)\"\n      }\n    }\n  })\n});\n\nconst { result } = await response.json();\nconsole.log(\"‚úÖ Execution result:\");\nconsole.log(JSON.stringify(result, null, 2));"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: LLM with Tool Calling\n\nNow let the LLM decide when to use tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Import LLM provider\nimport { createLLM, generateCompletion } from \"../examples/llm-provider.ts\";\nimport { generateText, tool } from \"npm:ai\";\nimport { z } from \"npm:zod\";\n\nconst apiKey = Deno.env.get(\"ANTHROPIC_API_KEY\") ||\n               Deno.env.get(\"OPENAI_API_KEY\") ||\n               Deno.env.get(\"GOOGLE_API_KEY\");\n\nif (!apiKey) {\n  throw new Error(\"Set an API key in llm-demo.ipynb first!\");\n}\n\nconst model = createLLM({ apiKey });\n\nconsole.log(\"‚úÖ LLM ready\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the tool for the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "const executeCodeTool = tool({\n  description: \"Execute TypeScript/JavaScript code safely via MCP\",\n  parameters: z.object({\n    code: z.string().describe(\"TypeScript/JavaScript code to execute\"),\n    context: z.any().optional().describe(\"Optional context data\")\n  }),\n  execute: async ({ code, context }) => {\n    const response = await fetch(\"http://localhost:3000/message\", {\n      method: \"POST\",\n      headers: { \"Content-Type\": \"application/json\" },\n      body: JSON.stringify({\n        jsonrpc: \"2.0\",\n        id: Math.random(),\n        method: \"tools/call\",\n        params: {\n          name: \"agentcards__agentcards_execute_code\",\n          arguments: { code, context }\n        }\n      })\n    });\n    const { result } = await response.json();\n    return result.content[0].text;\n  }\n});"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Ask LLM to Solve Task with Code\n\nThe LLM will generate and execute code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const result = await generateText({\n  model,\n  tools: { execute_code: executeCodeTool },\n  maxSteps: 5,\n  prompt: \"Calculate the sum of all prime numbers between 1 and 100. Write and execute TypeScript code to do this.\"\n});\n\nconsole.log(\"\\nü§ñ LLM Response:\");\nconsole.log(result.text);\n\nconsole.log(\"\\nüìä Tool Calls:\");\nfor (const step of result.steps) {\n  if (step.toolCalls) {\n    for (const call of step.toolCalls) {\n      console.log(`\\nüîß Called: ${call.toolName}`);\n      console.log(\"   Args:\", JSON.stringify(call.args, null, 2));\n      console.log(\"   Result:\", JSON.stringify(step.toolResults, null, 2));\n    }\n  }\n}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Multi-Step Workflow\n\nLLM orchestrates multiple tool calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const workflow = await generateText({\n  model,\n  tools: { execute_code: executeCodeTool },\n  maxSteps: 10,\n  prompt: `Analyze this sales data step by step:\n1. Calculate total revenue\n2. Find best selling product\n3. Calculate average price\n\nData: [{\"product\":\"Laptop\",\"price\":1200,\"qty\":2},{\"product\":\"Mouse\",\"price\":25,\"qty\":5},{\"product\":\"Keyboard\",\"price\":80,\"qty\":3}]\n\nWrite code to perform each calculation.`\n});\n\nconsole.log(\"\\nü§ñ Workflow Result:\");\nconsole.log(workflow.text);\n\nconsole.log(\"\\nüìà Steps executed:\", workflow.steps.length);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Cleanup\n\nStop the MCP server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serverProcess.kill(\"SIGTERM\");\nawait serverProcess.status;\nconsole.log(\"‚úÖ Server stopped\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n\n**What we demonstrated:**\n- ‚úÖ MCP server exposes tools via HTTP\n- ‚úÖ LLM discovers and calls tools automatically\n- ‚úÖ Safe code execution in sandbox\n- ‚úÖ Multi-step workflow orchestration\n- ‚úÖ LLM decides when and how to use tools\n\n**The MCP Pattern:**\n```\nUser Query ‚Üí LLM ‚Üí Tool Selection ‚Üí MCP Server ‚Üí Sandbox ‚Üí Results ‚Üí LLM ‚Üí User\n```\n\nThis is how Claude Code, Cline, and other AI coding tools work!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}