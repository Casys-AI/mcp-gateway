{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["# MCP Gateway with LLM Integration\n\nDemonstrates how LLMs use MCP tools to execute code safely.\n\n**What you'll learn:**\n- Start HTTP MCP server\n- LLM calls tools via MCP\n- Workflow orchestration with tool calling"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Step 1: Start HTTP MCP Server\n\nFirst, start the server in background:"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["// Start HTTP server in background\nconst server = new Deno.Command(\"deno\", {\n  args: [\"run\", \"--allow-all\", \"../examples/http-server.ts\"],\n  stdout: \"piped\",\n  stderr: \"piped\"\n});\n\nconst serverProcess = server.spawn();\n\n// Wait for server to be ready\nawait new Promise(r => setTimeout(r, 2000));\n\n// Check health\nconst health = await fetch(\"http://localhost:3000/health\");\nconst healthData = await health.json();\n\nconsole.log(\"âœ… MCP Server ready:\", healthData);"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Step 2: List Available Tools\n\nSee what tools the MCP server exposes:"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["const response = await fetch(\"http://localhost:3000/tools/list\", {\n  method: \"POST\"\n});\n\nconst { tools } = await response.json();\n\nconsole.log(`ðŸ“‹ Available tools (${tools.length}):`);\nfor (const tool of tools) {\n  console.log(`\\nðŸ”§ ${tool.name}`);\n  console.log(`   ${tool.description}`);\n}"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Step 3: Call Tool Directly\n\nTest tool execution without LLM:"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["const response = await fetch(\"http://localhost:3000/tools/call\", {\n  method: \"POST\",\n  headers: { \"Content-Type\": \"application/json\" },\n  body: JSON.stringify({\n    tool: \"execute_code\",\n    params: {\n      code: \"return Array.from({length: 10}, (_, i) => i * i)\"\n    }\n  })\n});\n\nconst result = await response.json();\nconsole.log(\"Result:\", JSON.stringify(result, null, 2));"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Step 4: LLM with Tool Calling\n\nNow let the LLM decide when to use tools:"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["// Import LLM provider\nimport { createLLM, generateCompletion } from \"../examples/llm-provider.ts\";\nimport { generateText, tool } from \"npm:ai\";\nimport { z } from \"npm:zod\";\n\nconst apiKey = Deno.env.get(\"ANTHROPIC_API_KEY\") ||\n               Deno.env.get(\"OPENAI_API_KEY\") ||\n               Deno.env.get(\"GOOGLE_API_KEY\");\n\nif (!apiKey) {\n  throw new Error(\"Set an API key in llm-demo.ipynb first!\");\n}\n\nconst model = createLLM({ apiKey });\n\nconsole.log(\"âœ… LLM ready\");"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["Define the tool for the LLM:"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["const executeCodeTool = tool({\n  description: \"Execute TypeScript/JavaScript code safely in a sandbox\",\n  parameters: z.object({\n    code: z.string().describe(\"TypeScript/JavaScript code to execute\"),\n    context: z.any().optional().describe(\"Optional context data\")\n  }),\n  execute: async ({ code, context }) => {\n    const response = await fetch(\"http://localhost:3000/tools/call\", {\n      method: \"POST\",\n      headers: { \"Content-Type\": \"application/json\" },\n      body: JSON.stringify({\n        tool: \"execute_code\",\n        params: { code, context }\n      })\n    });\n    const result = await response.json();\n    return result.result;\n  }\n});"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Step 5: Ask LLM to Solve Task with Code\n\nThe LLM will generate and execute code:"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["const result = await generateText({\n  model,\n  tools: { execute_code: executeCodeTool },\n  maxSteps: 5,\n  prompt: \"Calculate the sum of all prime numbers between 1 and 100. Write and execute TypeScript code to do this.\"\n});\n\nconsole.log(\"\\nðŸ¤– LLM Response:\");\nconsole.log(result.text);\n\nconsole.log(\"\\nðŸ“Š Tool Calls:\");\nfor (const step of result.steps) {\n  if (step.toolCalls) {\n    for (const call of step.toolCalls) {\n      console.log(`\\nðŸ”§ Called: ${call.toolName}`);\n      console.log(\"   Args:\", JSON.stringify(call.args, null, 2));\n      console.log(\"   Result:\", JSON.stringify(step.toolResults, null, 2));\n    }\n  }\n}"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Step 6: Multi-Step Workflow\n\nLLM orchestrates multiple tool calls:"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["const workflow = await generateText({\n  model,\n  tools: { execute_code: executeCodeTool },\n  maxSteps: 10,\n  prompt: `Analyze this sales data step by step:\n1. Calculate total revenue\n2. Find best selling product\n3. Calculate average price\n\nData: [{\"product\":\"Laptop\",\"price\":1200,\"qty\":2},{\"product\":\"Mouse\",\"price\":25,\"qty\":5},{\"product\":\"Keyboard\",\"price\":80,\"qty\":3}]\n\nWrite code to perform each calculation.`\n});\n\nconsole.log(\"\\nðŸ¤– Workflow Result:\");\nconsole.log(workflow.text);\n\nconsole.log(\"\\nðŸ“ˆ Steps executed:\", workflow.steps.length);"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Step 7: Cleanup\n\nStop the MCP server:"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["serverProcess.kill(\"SIGTERM\");\nawait serverProcess.status;\nconsole.log(\"âœ… Server stopped\");"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Summary\n\n**What we demonstrated:**\n- âœ… MCP server exposes tools via HTTP\n- âœ… LLM discovers and calls tools automatically\n- âœ… Safe code execution in sandbox\n- âœ… Multi-step workflow orchestration\n- âœ… LLM decides when and how to use tools\n\n**The MCP Pattern:**\n```\nUser Query â†’ LLM â†’ Tool Selection â†’ MCP Server â†’ Sandbox â†’ Results â†’ LLM â†’ User\n```\n\nThis is how Claude Code, Cline, and other AI coding tools work!"]
    }
  ],
  "metadata": {
    "kernelspec": {"display_name": "Deno", "language": "typescript", "name": "deno"},
    "language_info": {"file_extension": ".ts", "mimetype": "text/x.typescript", "name": "typescript"}
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
